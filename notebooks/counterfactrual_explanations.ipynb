{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterfactual Exmplanations implementation notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary stage:\n",
    "- prepare data (dataloader for one point)\n",
    "- prepare model (load from checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "data = pd.read_csv('../data/processed/validation.csv').drop(columns=['Unnamed: 0'])\n",
    "\n",
    "data = data[data['loan_status'] == 1]\n",
    "X = data.drop(columns=['loan_status'])\n",
    "y = data['loan_status']\n",
    "\n",
    "numerical_cols = [\n",
    "   'person_age','person_income','person_emp_length','loan_amnt',\n",
    "   'loan_int_rate','loan_percent_income','cb_person_cred_hist_length'\n",
    "]\n",
    "all_cols = list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected random point from negative class index: 713\n"
     ]
    }
   ],
   "source": [
    "random_idx = random.choice(range(data.shape[0]))\n",
    "print(f'selected random point from negative class index: {random_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X.iloc[random_idx].values, dtype=torch.float)\n",
    "y_tensor = torch.tensor(y.iloc[random_idx], dtype=torch.float)\n",
    "X_tensor = X_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CreditRiskModel(\n",
       "   (model): Sequential(\n",
       "     (0): Linear(in_features=23, out_features=64, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " device(type='mps', index=0))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "pos_weight = 2.0\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "\n",
    "class CreditRiskModel (pl.LightningModule):\n",
    "    def __init__(self, input_dim = 23, hidden = 64, sigmoid_threashold=0.5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "        self.threashold = sigmoid_threashold\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return logits.squeeze(1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        logits = self.forward(X)\n",
    "        # loss = F.binary_cross_entropy_with_logits(logits, y)\n",
    "        loss = criterion(logits, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        logits = self.forward(X)\n",
    "        # loss = F.binary_cross_entropy_with_logits(logits, y)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > self.threashold).float()\n",
    "\n",
    "        acc = (preds == y).float().mean()\n",
    "        preds_np = preds.detach().cpu().numpy()\n",
    "        y_np = y.detach().cpu().numpy()\n",
    "\n",
    "        fnr = 1. - recall_score(y_np, preds_np)\n",
    "\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_fnr', fnr, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "\n",
    "ckpt_path = '../models/model/lightning_logs/credit_risk_model/version_13/checkpoints/epoch=79-step=3600.ckpt'\n",
    "params_path= '../models/model/lightning_logs/credit_risk_model/version_13/hparams.yaml'\n",
    "model = CreditRiskModel.load_from_checkpoint(ckpt_path, hparams_file=params_path)\n",
    "model, model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla counterfactuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is:\n",
    "\n",
    "1. We have a **trained model** $f$ that maps inputs $\\mathbf{x}$ (the borrower’s attributes) to a **probability** of default (or a logit).  \n",
    "2. We want to find a **counterfactual** $\\mathbf{x}'$ such that the model’s prediction changes from the original class (e.g., “default”) to the **desired class** (e.g., “non-default”), while staying as **close** as possible to the original $\\mathbf{x}$.\n",
    "\n",
    "This is often framed as **optimization**:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{x}'} \\; d(\\mathbf{x}, \\mathbf{x}') + \\lambda \\,\\mathcal{L}\\bigl(f(\\mathbf{x}'), c\\bigr),\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $d(\\mathbf{x}, \\mathbf{x}')$ is a **distance** measure (L1, L2, etc.) that encourages $\\mathbf{x}'$ to remain similar to $\\mathbf{x}$.  \n",
    "- $\\mathcal{L}(f(\\mathbf{x}'), c)$ is a **classification loss** that forces the model’s prediction for $\\mathbf{x}'$ to be the **target class** $c$.  \n",
    "- $\\lambda$ balances how strongly we weigh prediction accuracy vs. closeness.\n",
    "\n",
    "> **Note**: This is a simplified gradient-based approach inspired by **Wachter et al. (2018)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_counterfactual(\n",
    "        model,\n",
    "        x_original,\n",
    "        target_label=0,    # 0 for \"non-default\", 1 for \"default\"\n",
    "        lambda_param=1.0,  # trade-off coefficient between distance & classification loss\n",
    "        lr=0.01,           # learning rate for gradient descent\n",
    "        max_steps=500,     # max optimization steps\n",
    "        distance_metric='l2' ):\n",
    "    x_original = x_original.to(model.device).detach()\n",
    "    x_cf = x_original.clone().requires_grad_(True)\n",
    "    optimizer = Adam([x_cf], lr=lr)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate distance: encourage counterfactual to be close to original\n",
    "        if distance_metric == 'l2':\n",
    "            distance = torch.norm(x_cf - x_original, p=2)\n",
    "        else : # use 'l1'\n",
    "            distance = torch.norm(torch.norm(x_cf - x_original, p=1))\n",
    "        \n",
    "        # Calculate classification loss of the cf from the model prediction\n",
    "        logits = model.forward(x_cf)\n",
    "        label_tensor = torch.tensor([float(target_label)]).to(x_cf.device)\n",
    "\n",
    "        # possible dimension problems...\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, label_tensor)\n",
    "\n",
    "        #Calculate main formula of the method\n",
    "        loss = distance + lambda_param * bce_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return x_cf.detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "1. **`x_cf`** is the **counterfactual** variable we’re optimizing. It starts as a copy of $\\mathbf{x}$.  \n",
    "2. We define an **Adam** optimizer with `x_cf` as its “parameter.” Each step does `loss.backward()` on `x_cf`.  \n",
    "3. **Distance**:  \n",
    "   - `distance = torch.norm(x_cf - x_original, p=2)` for **L2**.  \n",
    "   - You could switch to **p=1** for **L1** or a custom distance.  \n",
    "4. **Classification loss**: We want the model’s prediction for `x_cf` to have the probability of the positive class = `target_label`. We treat this as a binary cross-entropy objective, so if `target_label=0`, it tries to push logits negative; if `1`, it tries to push them positive.  \n",
    "5. **`loss = distance + lambda_param * bce_loss`**: We combine them, so the final solution tries to both “flip” the prediction and remain close to the original.  \n",
    "6. Return `x_cf.detach()` at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x: tensor([[0.0312, 0.0069, 0.0488, 0.1594, 0.0674, 0.4280, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.0000, 0.0000]])\n",
      "Counterfactual x': tensor([[ 3.5399e-02,  5.8824e-02,  5.0257e-02,  2.0245e-01,  5.7947e-02,\n",
      "          3.8079e-01,  1.9936e-03,  9.9923e-01,  1.2513e-04,  4.3102e-04,\n",
      "         -7.1766e-03, -4.7137e-03, -1.7670e-04,  1.0033e+00,  1.0017e+00,\n",
      "          3.9685e-03, -1.6695e-03, -5.5569e-03, -9.4725e-03, -3.9067e-03,\n",
      "         -3.5214e-02,  1.0016e+00,  1.5586e-04]])\n"
     ]
    }
   ],
   "source": [
    "x_cf = generate_counterfactual(\n",
    "    model=model,\n",
    "    x_original=X_tensor,\n",
    "    target_label=0,\n",
    "    lambda_param=0.5,\n",
    "    lr=0.01,\n",
    "    max_steps=500,\n",
    "    distance_metric='l2'\n",
    ")\n",
    "\n",
    "print(\"Original x:\", X_tensor)\n",
    "print(\"Counterfactual x':\", x_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result interpretation:\n",
    "We see that our vanilla counterfactual is woking conceptually correct\n",
    "\n",
    "Observations:\n",
    "1. **Most Values Are Similar**: Many features (especially the first half) only shifted slightly – for instance, `0.1562 → 0.1606`, `0.3415 → 0.2691`, etc. These small nudges can be enough to reduce the model’s predicted probability of default if those features have higher influence.\n",
    "\n",
    "2. **Negative Shifts**: Some features that were originally `0.0000` became slightly negative (e.g., `-0.0354`, `-0.0563`). Because we **didn’t impose any constraints** to keep them ≥0, the gradient-based optimization allowed the counterfactual to move continuously in that space. In a real credit scenario, it might make no sense to have negative values for certain features. You’d typically clamp them to valid ranges or use a custom penalty for going below zero.\n",
    "\n",
    "3. **Small Gains Above 1**: For instance, `1.0000 → 1.0163` or `1.0000 → 1.0030`. Again, without constraints or clamping, the counterfactual can become slightly >1. If a feature is fundamentally bounded in [0,1], you’d usually clamp or project it back within [0,1] after each gradient step.\n",
    "\n",
    "4. **Interpretation**: The underlying assumption is that by making these minor continuous changes, the model’s predicted outcome changes from “default likely” to “non-default.” In practice, you’d want to:\n",
    "   - Impose **feasibility** constraints (e.g., if some fields must remain integer or within [0,1]).  \n",
    "   - Possibly adjust the **distance function** or weighting $\\lambda$ if changes are too scattered or too large.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Add Constraints**: For example, clamp each feature after each gradient step so that `feature_value = feature_value.clamp(min=0.0, max=1.0)` if you know it’s a normalized ratio.  \n",
    "- **Domain Logic**: If some features are strictly categorical (e.g., one-hot vectors for “rent,” “own,” etc.), you could freeze them or only allow changes among valid one-hot states.  \n",
    "- **Tune $\\lambda$**: Adjust the trade-off parameter so that the model modifies fewer features or flips the prediction more decisively.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counterfactual exmplanations with a twist\n",
    "\n",
    "- Some columns like `loan_purpose`, should not be included into the analysis because one takes a loan for some reason and is not interested in changing this reason just to get the loan\n",
    "  - Out goal is to dive into some feature analysis and think what features should be frozen (unchanged during optimization)\n",
    "- Next, we need to add some contraints to the optimizer, on how the values should be changed:\n",
    "  - most of out features are OneHot encoded categorical features, so for example if `cb_person_default_on_file_Y` is set `0` -> `1`, then `cb_person_default_on_file_N` should automatically turn `1` -> `0`.\n",
    "  - Also we should say that from categorical encodings the change should be discrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>person_home_ownership</th>\n",
       "      <th>loan_intent_DEBTCONSOLIDATION</th>\n",
       "      <th>loan_intent_EDUCATION</th>\n",
       "      <th>...</th>\n",
       "      <th>loan_grade_A</th>\n",
       "      <th>loan_grade_B</th>\n",
       "      <th>loan_grade_C</th>\n",
       "      <th>loan_grade_D</th>\n",
       "      <th>loan_grade_E</th>\n",
       "      <th>loan_grade_F</th>\n",
       "      <th>loan_grade_G</th>\n",
       "      <th>cb_person_default_on_file_N</th>\n",
       "      <th>cb_person_default_on_file_Y</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.118944</td>\n",
       "      <td>0.022717</td>\n",
       "      <td>0.097325</td>\n",
       "      <td>0.312161</td>\n",
       "      <td>0.436917</td>\n",
       "      <td>0.323226</td>\n",
       "      <td>0.132439</td>\n",
       "      <td>1.287671</td>\n",
       "      <td>0.206285</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131346</td>\n",
       "      <td>0.246575</td>\n",
       "      <td>0.184529</td>\n",
       "      <td>0.298952</td>\n",
       "      <td>0.106366</td>\n",
       "      <td>0.019339</td>\n",
       "      <td>0.012893</td>\n",
       "      <td>0.683320</td>\n",
       "      <td>0.316680</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.103969</td>\n",
       "      <td>0.019156</td>\n",
       "      <td>0.092896</td>\n",
       "      <td>0.214208</td>\n",
       "      <td>0.183560</td>\n",
       "      <td>0.168896</td>\n",
       "      <td>0.150222</td>\n",
       "      <td>0.522325</td>\n",
       "      <td>0.404801</td>\n",
       "      <td>0.357886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337914</td>\n",
       "      <td>0.431191</td>\n",
       "      <td>0.388071</td>\n",
       "      <td>0.457984</td>\n",
       "      <td>0.308430</td>\n",
       "      <td>0.137770</td>\n",
       "      <td>0.112858</td>\n",
       "      <td>0.465369</td>\n",
       "      <td>0.465369</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.012771</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.312921</td>\n",
       "      <td>0.178806</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.018666</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>0.457865</td>\n",
       "      <td>0.317238</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.027508</td>\n",
       "      <td>0.146341</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>0.573034</td>\n",
       "      <td>0.445913</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.294727</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958427</td>\n",
       "      <td>0.919055</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        person_age  person_income  person_emp_length    loan_amnt  \\\n",
       "count  1241.000000    1241.000000        1241.000000  1241.000000   \n",
       "mean      0.118944       0.022717           0.097325     0.312161   \n",
       "std       0.103969       0.019156           0.092896     0.214208   \n",
       "min       0.015625       0.000491           0.000000     0.014493   \n",
       "25%       0.046875       0.012771           0.024390     0.130435   \n",
       "50%       0.093750       0.018666           0.073171     0.275362   \n",
       "75%       0.156250       0.027508           0.146341     0.420290   \n",
       "max       0.781250       0.294727           0.731707     1.000000   \n",
       "\n",
       "       loan_int_rate  loan_percent_income  cb_person_cred_hist_length  \\\n",
       "count    1241.000000          1241.000000                 1241.000000   \n",
       "mean        0.436917             0.323226                    0.132439   \n",
       "std         0.183560             0.168896                    0.150222   \n",
       "min         0.000000             0.008676                    0.000000   \n",
       "25%         0.312921             0.178806                    0.035714   \n",
       "50%         0.457865             0.317238                    0.071429   \n",
       "75%         0.573034             0.445913                    0.214286   \n",
       "max         0.958427             0.919055                    1.000000   \n",
       "\n",
       "       person_home_ownership  loan_intent_DEBTCONSOLIDATION  \\\n",
       "count            1241.000000                    1241.000000   \n",
       "mean                1.287671                       0.206285   \n",
       "std                 0.522325                       0.404801   \n",
       "min                 0.000000                       0.000000   \n",
       "25%                 1.000000                       0.000000   \n",
       "50%                 1.000000                       0.000000   \n",
       "75%                 2.000000                       0.000000   \n",
       "max                 3.000000                       1.000000   \n",
       "\n",
       "       loan_intent_EDUCATION  ...  loan_grade_A  loan_grade_B  loan_grade_C  \\\n",
       "count            1241.000000  ...   1241.000000   1241.000000   1241.000000   \n",
       "mean                0.150685  ...      0.131346      0.246575      0.184529   \n",
       "std                 0.357886  ...      0.337914      0.431191      0.388071   \n",
       "min                 0.000000  ...      0.000000      0.000000      0.000000   \n",
       "25%                 0.000000  ...      0.000000      0.000000      0.000000   \n",
       "50%                 0.000000  ...      0.000000      0.000000      0.000000   \n",
       "75%                 0.000000  ...      0.000000      0.000000      0.000000   \n",
       "max                 1.000000  ...      1.000000      1.000000      1.000000   \n",
       "\n",
       "       loan_grade_D  loan_grade_E  loan_grade_F  loan_grade_G  \\\n",
       "count   1241.000000   1241.000000   1241.000000   1241.000000   \n",
       "mean       0.298952      0.106366      0.019339      0.012893   \n",
       "std        0.457984      0.308430      0.137770      0.112858   \n",
       "min        0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       cb_person_default_on_file_N  cb_person_default_on_file_Y  loan_status  \n",
       "count                  1241.000000                  1241.000000       1241.0  \n",
       "mean                      0.683320                     0.316680          1.0  \n",
       "std                       0.465369                     0.465369          0.0  \n",
       "min                       0.000000                     0.000000          1.0  \n",
       "25%                       0.000000                     0.000000          1.0  \n",
       "50%                       1.000000                     0.000000          1.0  \n",
       "75%                       1.000000                     1.000000          1.0  \n",
       "max                       1.000000                     1.000000          1.0  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['person_age', 'person_income', 'person_emp_length', 'loan_amnt',\n",
      "       'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length',\n",
      "       'person_home_ownership', 'loan_intent_DEBTCONSOLIDATION',\n",
      "       'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT',\n",
      "       'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE',\n",
      "       'loan_grade_A', 'loan_grade_B', 'loan_grade_C', 'loan_grade_D',\n",
      "       'loan_grade_E', 'loan_grade_F', 'loan_grade_G',\n",
      "       'cb_person_default_on_file_N', 'cb_person_default_on_file_Y',\n",
      "       'loan_status'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature outline:\n",
    "\n",
    "| **Index** | **Feature**                       | **Type**      | **Can it be changed?**                                                        | **Range / Values**        | **Notes**                                                                                                           |\n",
    "|-----------|-----------------------------------|---------------|-------------------------------------------------------------------------------|---------------------------|----------------------------------------------------------------------------------------------------------------------|\n",
    "| **0**     | **person_age**                    | Numerical     | **Cannot be changed**                                                         | [0, 1] (scaled)           | Represents the borrower’s age. Typically viewed as fixed (e.g., for counterfactuals, we assume one cannot “get younger”). |\n",
    "| **1**     | **person_income**                 | Numerical     | **Can be changed**                                                            | [0, 1] (scaled)           | An applicant could theoretically change their declared (or actual) income or pursue higher income for a new application. |\n",
    "| **2**     | **person_emp_length**             | Numerical     | **Can be changed**                                                            | [0, 1] (scaled)           | Reflects length of employment; might be interpreted as adjusting tenure or switching jobs (though not trivial).       |\n",
    "| **3**     | **loan_amnt**                     | Numerical     | **Can be changed** (often negotiable)                                         | [0, 1] (scaled)           | The principal loan amount requested. In practice, one could apply for a smaller (or larger) loan.                     |\n",
    "| **4**     | **loan_int_rate**                 | Numerical     | **Cannot be changed**                                                         | [0, 1] (scaled)           | Interest rate is typically set by the lender based on risk; not directly changed by the applicant.                    |\n",
    "| **5**     | **loan_percent_income**           | Numerical     | **Indirectly changes if income or loan amount changes** (not directly changed) | [0, 1] (scaled)           | This ratio depends on both `person_income` and `loan_amnt`. Changing either will alter this feature automatically.     |\n",
    "| **6**     | **cb_person_cred_hist_length**    | Numerical     | **Cannot be changed**                                                         | [0, 1] (scaled)           | Represents the length of credit history on the applicant’s credit bureau report—typically not alterable immediately.   |\n",
    "| **7**     | **person_home_ownership**         | Categorical   | **Can be changed** (though not trivial in reality)                            | [0, 1, 2, 3] (ordinal)    | Categories usually represent: 0=RENT, 1=MORTGAGE, 2=OWN, 3=OTHER (or similar). Changing homeownership status may be complex. |\n",
    "| **8**     | **loan_intent_DEBTCONSOLIDATION** | Categorical   | **Cannot be changed**                                                         | [0, 1]                     | One-hot encoding for loan intent = debt consolidation. Typically a declared purpose of the loan.                       |\n",
    "| **9**     | **loan_intent_EDUCATION**         | Categorical   | **Cannot be changed**                                                         | [0, 1]                     | One-hot encoding for loan intent = education.                                                                         |\n",
    "| **10**    | **loan_intent_HOMEIMPROVEMENT**   | Categorical   | **Cannot be changed**                                                         | [0, 1]                     | One-hot encoding for loan intent = home improvement.                                                                  |\n",
    "| **11**    | **loan_intent_MEDICAL**           | Categorical   | **Cannot be changed**                                                         | [0, 1]                     | One-hot encoding for loan intent = medical.                                                                           |\n",
    "| **12**    | **loan_intent_PERSONAL**          | Categorical   | **Cannot be changed**                                                         | [0, 1]                     | One-hot encoding for loan intent = personal.                                                                          |\n",
    "| **13**    | **loan_intent_VENTURE**           | Categorical   | **Cannot be changed**                                                         | [0, 1]                     | One-hot encoding for loan intent = venture.                                                                           |\n",
    "| **14**    | **loan_grade_A**                  | Categorical   | **Cannot be changed**                                                         | [0, 1]                     | One-hot encoding for a particular loan grade assigned by the lender.                                                 |\n",
    "| **15**    | **loan_grade_B**                  | Categorical   | **Cannot be changed**                                                         | [0, 1]                     |                                                                                                                      |\n",
    "| **16**    | **loan_grade_C**                  | Categorical   | **Cannot be changed**                                                         | [0, 1]                     |                                                                                                                      |\n",
    "| **17**    | **loan_grade_D**                  | Categorical   | **Cannot be changed**                                                         | [0, 1]                     |                                                                                                                      |\n",
    "| **18**    | **loan_grade_E**                  | Categorical   | **Cannot be changed**                                                         | [0, 1]                     |                                                                                                                      |\n",
    "| **19**    | **loan_grade_F**                  | Categorical   | **Cannot be changed**                                                         | [0, 1]                     |                                                                                                                      |\n",
    "| **20**    | **loan_grade_G**                  | Categorical   | **Cannot be changed**                                                         | [0, 1]                     |                                                                                                                      |\n",
    "| **21**    | **cb_person_default_on_file_N**   | Categorical   | **Cannot be changed**                                                         | [0, 1]                     | Whether the person has a default on file (N=No).                                                                      |\n",
    "| **22**    | **cb_person_default_on_file_Y**   | Categorical   | **Cannot be changed**                                                         | [0, 1]                     | Whether the person has a default on file (Y=Yes).                                                                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_indices = [1, 2, 3, 7]\n",
    "fixed_indices = [i for i in range(X_tensor.shape[1]) if i not in free_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_mask(tensor, fixed_indices):\n",
    "    mask = torch.ones_like(tensor)\n",
    "    for idx in fixed_indices:\n",
    "        mask[0, idx] = 0.\n",
    "    return mask\n",
    "\n",
    "generate_mask(X_tensor, fixed_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masked_counterfactual(\n",
    "        model,\n",
    "        x_original,\n",
    "        mask,              # a mask for the gradients to fix the non changable features\n",
    "        target_label=0,    # 0 for \"non-default\", 1 for \"default\"\n",
    "        lambda_param=1.0,  # trade-off coefficient between distance & classification loss\n",
    "        lr=0.01,           # learning rate for gradient descent\n",
    "        max_steps=500,     # max optimization steps\n",
    "        distance_metric='l2' ):\n",
    "\n",
    "    x_original = x_original.to(model.device).detach()\n",
    "    x_cf = x_original.clone().requires_grad_(True)\n",
    "    mask = mask.to(model.device)\n",
    "    optimizer = Adam([x_cf], lr=lr)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate distance: encourage counterfactual to be close to original\n",
    "        if distance_metric == 'l2':\n",
    "            distance = torch.norm(x_cf - x_original, p=2)\n",
    "        else : # use 'l1'\n",
    "            distance = torch.norm(torch.norm(x_cf - x_original, p=1))\n",
    "        \n",
    "        # Calculate classification loss of the cf from the model prediction\n",
    "        logits = model.forward(x_cf)\n",
    "        label_tensor = torch.tensor([float(target_label)]).to(x_cf.device)\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, label_tensor)\n",
    "\n",
    "        #Calculate main formula of the method\n",
    "        loss = distance + lambda_param * bce_loss\n",
    "\n",
    "        loss.backward()\n",
    "        # Zero out gradients on frozen features\n",
    "        with torch.no_grad():\n",
    "            x_cf.grad *= mask\n",
    "        optimizer.step()\n",
    "\n",
    "    return x_cf.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = generate_mask(X_tensor, fixed_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_counterfactual_visuals(x_original, x_cf):\n",
    "    comparison_df = pd.DataFrame({\n",
    "    'Feature': all_cols,\n",
    "    'Original': x_original.squeeze().numpy(),\n",
    "    'Counterfactual': x_cf.squeeze().numpy()\n",
    "    })\n",
    "    comparison_df['delta'] = comparison_df['Counterfactual'] - comparison_df['Original']\n",
    "    return comparison_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Feature  Original  Counterfactual     delta\n",
      "0                      person_age  0.031250        0.031250  0.000000\n",
      "1                   person_income  0.006877        0.112425  0.105548\n",
      "2               person_emp_length  0.048780        0.049547  0.000767\n",
      "3                       loan_amnt  0.159420        0.222083  0.062663\n",
      "4                   loan_int_rate  0.067416        0.067416  0.000000\n",
      "5             loan_percent_income  0.428011        0.428011  0.000000\n",
      "6      cb_person_cred_hist_length  0.000000        0.000000  0.000000\n",
      "7           person_home_ownership  1.000000        1.017331  0.017331\n",
      "8   loan_intent_DEBTCONSOLIDATION  0.000000        0.000000  0.000000\n",
      "9           loan_intent_EDUCATION  0.000000        0.000000  0.000000\n",
      "10    loan_intent_HOMEIMPROVEMENT  0.000000        0.000000  0.000000\n",
      "11            loan_intent_MEDICAL  0.000000        0.000000  0.000000\n",
      "12           loan_intent_PERSONAL  0.000000        0.000000  0.000000\n",
      "13            loan_intent_VENTURE  1.000000        1.000000  0.000000\n",
      "14                   loan_grade_A  1.000000        1.000000  0.000000\n",
      "15                   loan_grade_B  0.000000        0.000000  0.000000\n",
      "16                   loan_grade_C  0.000000        0.000000  0.000000\n",
      "17                   loan_grade_D  0.000000        0.000000  0.000000\n",
      "18                   loan_grade_E  0.000000        0.000000  0.000000\n",
      "19                   loan_grade_F  0.000000        0.000000  0.000000\n",
      "20                   loan_grade_G  0.000000        0.000000  0.000000\n",
      "21    cb_person_default_on_file_N  1.000000        1.000000  0.000000\n",
      "22    cb_person_default_on_file_Y  0.000000        0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "x_cf = generate_masked_counterfactual(\n",
    "    model=model,\n",
    "    mask=mask,\n",
    "    x_original=X_tensor,\n",
    "    target_label=0,\n",
    "    lambda_param=0.5,\n",
    "    lr=0.01,\n",
    "    max_steps=500,\n",
    "    distance_metric='l2'\n",
    ")\n",
    "comparison_df = generate_counterfactual_visuals(X_tensor, x_cf)\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to further complexify or couterfactuals, because we have a computation feature called `loan_percent_income`\n",
    "\n",
    "`loan_percent_income` = `loan_amnt` / `person_income`\n",
    "\n",
    "this means if we modify either of those 2 featues the loan persent income feature will also change, right now it is freezed, but we need to incorporate it's recalculation every step of the of the optimization process, however it willl still be freezed. So we just recalculate it to make the model prediction more accurate, rather than optimize it directly. \n",
    "\n",
    "But firstly we need to check whether the calculation formula is correct:\n",
    "```py\n",
    "assert np.allclose(data['loan_amnt'] / data['person_income'], data['loan_percent_income'])\n",
    "```\n",
    "\n",
    "this gave us\n",
    "\n",
    "``` txt\n",
    "---------------------------------------------------------------------------\n",
    "AssertionError                            Traceback (most recent call last)\n",
    "Cell In[71], line 1\n",
    "----> 1 assert np.allclose(data['loan_amnt'] / data['person_income'], data['loan_percent_income'])\n",
    "```\n",
    "\n",
    "#### Why the Original Relationship Breaks\n",
    "\n",
    "We have:\n",
    "\n",
    "$$\n",
    "\\text{loan\\_percent\\_income} \\stackrel{\\text{ideal}}{=} \\frac{\\text{loan\\_amnt}}{\\text{person\\_income}},\n",
    "$$\n",
    "\n",
    "but once we apply a **MinMaxScaler** to each feature independently, each is scaled to $[0,1]$ using its own $\\min$ and $\\max$. Concretely:\n",
    "\n",
    "$$\n",
    "\\text{scaled\\_amnt} \n",
    "= \\frac{\\text{loan\\_amnt} - \\min(\\text{amnt})}{\\max(\\text{amnt}) - \\min(\\text{amnt})},\n",
    "\\quad\n",
    "\\text{scaled\\_income} \n",
    "= \\frac{\\text{person\\_income} - \\min(\\text{income})}{\\max(\\text{income}) - \\min(\\text{income})},\n",
    "\\quad \\dots\n",
    "$$\n",
    "\n",
    "Therefore, in **scaled** space, the ratio\n",
    "\n",
    "$$\n",
    "\\frac{\\text{scaled\\_amnt}}{\\text{scaled\\_income}}\n",
    "$$\n",
    "\n",
    "no longer matches the scaled value of $\\text{loan\\_percent\\_income}$. Hence, the direct relationship $\\text{loan\\_amnt} / \\text{person\\_income}$ is **broken** by independent scaling.\n",
    "\n",
    "---\n",
    "\n",
    "#### Recomputing the True Ratio\n",
    "\n",
    "If we **modify** `loan_amnt` or `person_income` during a process like counterfactual optimization, we likely want **`loan_percent_income`** to reflect the **true** ratio in original domain:\n",
    "\n",
    "$$\n",
    "\\text{orig\\_ratio} \n",
    "= \\frac{\\text{orig\\_loan\\_amnt}}{\\text{orig\\_person\\_income}}.\n",
    "$$\n",
    "\n",
    "However, because the model expects scaled inputs, we must perform these steps:\n",
    "\n",
    "1. **Inverse-transform** the two columns of interest $\\text{loan\\_amnt}$ and $\\text{person\\_income}$ from $[0,1]$ back to their original domain.\n",
    "2. Compute $\\text{new\\_ratio} = \\frac{\\text{orig\\_loan\\_amnt}}{\\text{orig\\_person\\_income}}$.\n",
    "3. **Re-scale** only that ratio column so it again lies in $[0,1]$.\n",
    "4. Inject the new scaled ratio back into the feature vector.\n",
    "\n",
    "---\n",
    "\n",
    "#### The Partial “Injection” Approach\n",
    "\n",
    "A naive method might inverse-transform **all** numeric columns, recalc the ratio, and re-transform everything. That can introduce minor float changes to columns we do not wish to alter (like `person_age`). Instead, we do a **partial** approach, often called “injection”:\n",
    "\n",
    "1. **Extract** the scaled columns for **A** = `person_income` and **B** = `loan_amnt`.  \n",
    "2. **Inverse** them individually:\n",
    "\n",
    "   $$\n",
    "   \\text{origA}\n",
    "   = (\\text{scaledA}) \\times (\\max(A) - \\min(A)) \n",
    "      \\;+\\; \\min(A),\n",
    "   \\quad\n",
    "   \\text{origB}\n",
    "   = (\\text{scaledB}) \\times (\\max(B) - \\min(B)) \n",
    "      \\;+\\; \\min(B).\n",
    "   $$\n",
    "\n",
    "3. **Compute** $C$ in the original domain, e.g. for a ratio:\n",
    "   $$\n",
    "   C = \n",
    "   \\begin{cases}\n",
    "     \\frac{B}{A}, & A \\neq 0, \\\\\n",
    "     0.0, & A = 0.\n",
    "   \\end{cases}\n",
    "   $$\n",
    "4. **Scale** only $C$ back to $[0,1]$:\n",
    "\n",
    "   $$\n",
    "   \\text{scaledC}\n",
    "   = \\frac{\\text{origC} - \\min(C)}{\\max(C) - \\min(C)},\n",
    "   \\quad\n",
    "   \\text{(then optionally clamp in [0,1])}.\n",
    "   $$\n",
    "\n",
    "5. **Inject** $\\text{scaledC}$ back into the feature vector at the index of `loan_percent_income`. We do **not** alter the scaled $\\text{loan\\_amnt}$ or $\\text{person\\_income}$, leaving them exactly as is if we haven’t otherwise changed them.\n",
    "\n",
    "Thus, if we are only re-calculating `loan_percent_income`, no other feature experiences a float round-trip. This is beneficial in a **counterfactual** scenario, where we typically want fine control over which columns truly update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_recalc_dict(\n",
    "        A_name, B_name, C_name, ratio_func, scaler):\n",
    "    \"\"\"\n",
    "    Create a dictionary of parameters needed to recalculate a scaled feature C\n",
    "    based on a relationship between two other scaled features A and B \n",
    "    (e.g., C = B / A in the original domain).\n",
    "\n",
    "    Args:\n",
    "        A_name (str): The column name for component A (e.g., 'person_income').\n",
    "        B_name (str): The column name for component B (e.g., 'loan_amnt').\n",
    "        C_name (str): The column name for component C (e.g., 'loan_percent_income').\n",
    "        ratio_func (callable): A function ratio_func(a, b) -> float, defining how\n",
    "            to compute the original-domain value of C from A and B.\n",
    "        scaler (MinMaxScaler): A fitted MinMaxScaler that was trained on the \n",
    "            numeric columns, used for inverse transforming A/B and re-scaling C.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the global/local indices for A, B, C,\n",
    "              plus the calculation function and the scaler.\n",
    "              This dictionary is meant to be passed to `recalculate_scaled()`.\n",
    "    \n",
    "    Notes:\n",
    "        - Assumes you have global variables `all_cols` and `numerical_cols`\n",
    "          which define the order of the full feature set and the numeric subset, \n",
    "          respectively.\n",
    "    \"\"\"\n",
    "    idxA_global = all_cols.index(A_name)\n",
    "    idxB_global = all_cols.index(B_name)\n",
    "    idxC_global = all_cols.index(C_name)\n",
    "\n",
    "    idxA_local = numerical_cols.index(A_name)\n",
    "    idxB_local = numerical_cols.index(B_name)   \n",
    "    idxC_local = numerical_cols.index(C_name)\n",
    "    return {\n",
    "        'global': [idxA_global, idxB_global, idxC_global],\n",
    "        'local': [idxA_local, idxB_local, idxC_local],\n",
    "        'calc_func': ratio_func,\n",
    "        'scaler': scaler\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def recalculate_scaled(\n",
    "    x_scaled_full: torch.Tensor,\n",
    "    recacl_param_dict: dict\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Update one scaled feature (C) in x_scaled_full based on a user-defined\n",
    "    relationship involving two other scaled features (A and B). Only the \n",
    "    feature C column is changed; all other values remain identical.\n",
    "\n",
    "    Args:\n",
    "        x_scaled_full (torch.Tensor): A tensor of shape [1, num_features],\n",
    "            containing scaled values for the entire feature vector.\n",
    "        recacl_param_dict (dict): A dictionary returned by `form_recalc_dict()`, \n",
    "            containing:\n",
    "            - 'global': list of [idxA_global, idxB_global, idxC_global] \n",
    "              for the feature indices in the full input.\n",
    "            - 'local': list of [idxA_local, idxB_local, idxC_local] \n",
    "              for their indices in the numeric array used by the scaler.\n",
    "            - 'calc_func': a function calc_func(a, b) -> c in the original domain.\n",
    "            - 'scaler': the fitted MinMaxScaler used to do partial inverse.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A new tensor of shape [1, num_features], where only\n",
    "            the feature C column has been recalculated and re-scaled \n",
    "            based on the relationship between A and B in the original domain.\n",
    "\n",
    "    Notes:\n",
    "        - The function does not alter A or B themselves, nor any other columns.\n",
    "        - This approach ensures only the target column C is re-scaled.\n",
    "        - The user can define any function for `calc_func(a, b)` to represent \n",
    "          the original-domain relationship (e.g., c = b / a).\n",
    "    \"\"\"\n",
    "    device = x_scaled_full.device\n",
    "    dtype  = x_scaled_full.dtype\n",
    "\n",
    "    # Unpack recalculation params\n",
    "    idxA_global, idxB_global, idxC_global = recacl_param_dict['global']\n",
    "    idxA_local, idxB_local, idxC_local = recacl_param_dict['local']\n",
    "    calc_func = recacl_param_dict['calc_func']\n",
    "    scaler = recacl_param_dict['scaler']\n",
    "    \n",
    "    x_cpu = x_scaled_full.clone().detach().cpu().numpy()\n",
    "    \n",
    "    scaledA = x_cpu[0, idxA_global]\n",
    "    scaledB = x_cpu[0, idxB_global]\n",
    "    \n",
    "    # Retrieve the min/max from scaler for local indices\n",
    "    minA = scaler.data_min_[idxA_local]\n",
    "    maxA = scaler.data_max_[idxA_local]\n",
    "    minB = scaler.data_min_[idxB_local]\n",
    "    maxB = scaler.data_max_[idxB_local]\n",
    "    minC = scaler.data_min_[idxC_local]\n",
    "    maxC = scaler.data_max_[idxC_local]\n",
    "\n",
    "    # Convert scaled -> original domain\n",
    "    #    orig_val = scaled_val * (max - min) + min\n",
    "    origA = scaledA * (maxA - minA) + minA\n",
    "    origB = scaledB * (maxB - minB) + minB\n",
    "\n",
    "    # Compute new C in the original domain\n",
    "    origC = calc_func(origA, origB)\n",
    "\n",
    "    # Re-scale that new C to [0,1]\n",
    "    #    scaledC = (origC - minC) / (maxC - minC)\n",
    "    denomC = (maxC - minC)\n",
    "    if denomC == 0:\n",
    "        scaledC = 0.0\n",
    "    else:\n",
    "        scaledC = (origC - minC) / denomC\n",
    "    # optional clamp\n",
    "    scaledC = max(0.0, min(1.0, scaledC))\n",
    "\n",
    "    # Inject new scaledC back\n",
    "    x_cpu[0, idxC_global] = scaledC\n",
    "\n",
    "    x_updated = torch.tensor(x_cpu, device=device, dtype=dtype)\n",
    "    return x_updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "import joblib\n",
    "\n",
    "scaler_path = '../models/scaler/scaler.pkl'\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "def ratio_func(a, b):\n",
    "    return 0.0 if a == 0.0 else (b / a)\n",
    "\n",
    "A_name = 'person_income'\n",
    "B_name = 'loan_amnt'\n",
    "C_name = 'loan_percent_income'\n",
    "\n",
    "\n",
    "recalc_params = form_recalc_dict(A_name, B_name, C_name, ratio_func, scaler)\n",
    "\n",
    "X_recalc = recalculate_scaled(X_tensor, recalc_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(X_tensor, X_recalc, rtol=1e-05, atol=1e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we comapare now the recalculated and original, nothing should have changed because the precomputed feature on the initial stage is correct (we computed it manually duraing data processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally can incorporate this into our counterfactual explanation generaton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we have multiple recalculations, we need to build the sort of matrix of feature relationships:\n",
    "```txt\n",
    "[\n",
    "    [A_name, B_name, C_name, calc_func1],\n",
    "    [D_name, E_name, F_name, calc_func2],\n",
    "    [...],\n",
    "    ...\n",
    "]\n",
    "```\n",
    "> **Note:** here we assume that scaler for numerical features is all the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalc_params_matrix = [['person_income', 'loan_amnt', 'loan_percent_income', ratio_func]] # in our case only one row in the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to put our recalculations after the optimizer step(tweeking the features), because during the next iteration the recalculated feature would effect the prediction, but then it is masked, so it does not directly affect the optimization.\n",
    "\n",
    "Step-by-Step Logic\n",
    "\n",
    "1. **Initialization**:  \n",
    "   - We have `x_original` shaped `[1, features]`.  \n",
    "   - We clone it into `x_cf` with `requires_grad_(True)`.  \n",
    "   - Our **mask** is a vector `[features]` with 1.0 for columns we allow to move, and 0.0 for columns we want to freeze or handle externally (like $C$).\n",
    "\n",
    "2. **Optimizer Loop**:\n",
    "\n",
    "   1. **Forward pass** to compute:\n",
    "      - A **distance** term: $\\text{x\\_cf} - \\text{x\\_original}$.  \n",
    "      - A **classification** loss: $\\mathrm{BCEWithLogitsLoss}(\\mathrm{logits}(x_{cf}), \\text{target\\_label})$.  \n",
    "      - **Combined**: $\\mathrm{loss} = \\mathrm{distance} + \\lambda \\times \\mathrm{bce\\_loss}$.\n",
    "\n",
    "   2. **backward()**: \n",
    "      - PyTorch computes gradients for **all** columns in `x_cf`.\n",
    "      - We then **zero out** the gradient for masked columns. Specifically, `x_cf.grad *= mask`. \n",
    "        - This ensures the optimizer does **not** modify $C$ directly.\n",
    "\n",
    "   3. **optimizer.step()**:\n",
    "      - Only columns with non-zero gradient (mask=1) get updated.  \n",
    "      - This moves $A$ and $B$ (and possibly other free columns).\n",
    "\n",
    "   4. **Manual injection of $C$**:\n",
    "      - After the step, we recompute $C$ from the newly updated $A$ and $B$.  \n",
    "      - We do this by **partial inverse** (to get original domain $A, B$) $\\to$ compute new $C$ $\\to$ scale new $C$ $\\to$ overwrite the “C” slot in `x_cf`.  \n",
    "      - Because `x_cf` has `requires_grad_(True)`, we must do this in a `with torch.no_grad()` block or break the gradient. But that’s intentional—we do **not** want the model to backprop through the formula for $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masked_counterfactual_with_recalculation(\n",
    "        model,\n",
    "        x_original,\n",
    "        mask,                 # a mask for the gradients to fix the non changable features\n",
    "        scaler,               # scaler we used to scale original data during processing\n",
    "        recalc_params_matrix, # matrix of feature relations needed recalculation \n",
    "        target_label=0,       # 0 for \"non-default\", 1 for \"default\"\n",
    "        lambda_param=1.0,     # trade-off coefficient between distance & classification loss\n",
    "        lr=0.01,              # learning rate for gradient descent\n",
    "        max_steps=500,        # max optimization steps\n",
    "        distance_metric='l2' ):\n",
    "\n",
    "    x_original = x_original.to(model.device).detach()\n",
    "    x_cf = x_original.clone().requires_grad_(True)\n",
    "    mask = mask.to(model.device)\n",
    "    optimizer = Adam([x_cf], lr=lr)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate distance: encourage counterfactual to be close to original\n",
    "        if distance_metric == 'l2':\n",
    "            distance = torch.norm(x_cf - x_original, p=2)\n",
    "        else : # use 'l1'\n",
    "            distance = torch.norm(torch.norm(x_cf - x_original, p=1))\n",
    "        \n",
    "        # Calculate classification loss of the cf from the model prediction\n",
    "        logits = model.forward(x_cf)\n",
    "        label_tensor = torch.tensor([float(target_label)]).to(x_cf.device)\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, label_tensor)\n",
    "\n",
    "        #Calculate main formula of the method\n",
    "        loss = distance + lambda_param * bce_loss\n",
    "\n",
    "        loss.backward()\n",
    "        # Zero out gradients on frozen features\n",
    "        with torch.no_grad():\n",
    "            x_cf.grad *= mask\n",
    "        optimizer.step()\n",
    "\n",
    "        # Recompute relationships after the optimization step\n",
    "        with torch.no_grad():\n",
    "            # Iterate through relationship matrix to do the recalculation\n",
    "            for rel in recalc_params_matrix:\n",
    "                A, B, C, func = rel\n",
    "                param_dict = form_recalc_dict(A, B, C, func, scaler)\n",
    "                x_cf_updated = recalculate_scaled(x_cf, param_dict)\n",
    "                x_cf.copy_(x_cf_updated)\n",
    "\n",
    "    return x_cf.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = generate_mask(X_tensor, fixed_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Feature  Original  Counterfactual     delta\n",
      "0                      person_age  0.031250        0.031250  0.000000\n",
      "1                   person_income  0.006877        0.018195  0.011318\n",
      "2               person_emp_length  0.048780        0.047478 -0.001303\n",
      "3                       loan_amnt  0.159420        0.170310  0.010890\n",
      "4                   loan_int_rate  0.067416        0.067416  0.000000\n",
      "5             loan_percent_income  0.428011        0.198590 -0.229420\n",
      "6      cb_person_cred_hist_length  0.000000        0.000000  0.000000\n",
      "7           person_home_ownership  1.000000        0.998743 -0.001257\n",
      "8   loan_intent_DEBTCONSOLIDATION  0.000000        0.000000  0.000000\n",
      "9           loan_intent_EDUCATION  0.000000        0.000000  0.000000\n",
      "10    loan_intent_HOMEIMPROVEMENT  0.000000        0.000000  0.000000\n",
      "11            loan_intent_MEDICAL  0.000000        0.000000  0.000000\n",
      "12           loan_intent_PERSONAL  0.000000        0.000000  0.000000\n",
      "13            loan_intent_VENTURE  1.000000        1.000000  0.000000\n",
      "14                   loan_grade_A  1.000000        1.000000  0.000000\n",
      "15                   loan_grade_B  0.000000        0.000000  0.000000\n",
      "16                   loan_grade_C  0.000000        0.000000  0.000000\n",
      "17                   loan_grade_D  0.000000        0.000000  0.000000\n",
      "18                   loan_grade_E  0.000000        0.000000  0.000000\n",
      "19                   loan_grade_F  0.000000        0.000000  0.000000\n",
      "20                   loan_grade_G  0.000000        0.000000  0.000000\n",
      "21    cb_person_default_on_file_N  1.000000        1.000000  0.000000\n",
      "22    cb_person_default_on_file_Y  0.000000        0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "x_cf = generate_masked_counterfactual_with_recalculation(\n",
    "    model=model,\n",
    "    mask=mask,\n",
    "    x_original=X_tensor,\n",
    "    scaler=scaler,\n",
    "    recalc_params_matrix=recalc_params_matrix,\n",
    "    target_label=0,\n",
    "    lambda_param=0.5,\n",
    "    lr=0.01,\n",
    "    max_steps=500,\n",
    "    distance_metric='l2'\n",
    ")\n",
    "comparison_df = generate_counterfactual_visuals(X_tensor, x_cf)\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
